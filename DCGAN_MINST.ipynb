{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float)\n",
    "y_train = y_train.astype(np.float)\n",
    "x_train = x_train/np.max(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first four images of x_train and their own labels are in the following:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEYCAYAAAA6b7/5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZiU8UJqZlfBAaWZW4v8Damyp9PEULlAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118d269e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The first four images of x_train and their own labels are in the following:\")\n",
    "f, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\n",
    "ax1.imshow(x_train[0], cmap='Greys',  interpolation='nearest')\n",
    "ax1.set_title('image label is {}'.format(y_train[0]))\n",
    "ax2.imshow(x_train[1], cmap='Greys',  interpolation='nearest')\n",
    "ax2.set_title('image label is {}'.format(y_train[1]))\n",
    "ax3.imshow(x_train[2], cmap='Greys',  interpolation='nearest')\n",
    "ax3.set_title('image label is {}'.format(y_train[2]))\n",
    "ax4.imshow(x_train[3], cmap='Greys',  interpolation='nearest')\n",
    "ax4.set_title('image label is {}'.format(y_train[3]))\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (W−F+2P)/S+1 : larger than convoulution window: /2 else: keep\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        \n",
    "        # Input size: 28 x 28 x 1, depth = 1\n",
    "        # Conv2D(<#out featuere map>,<filter size>,<stride>,<input>,<padding>)\n",
    "        self.D.add(Conv2D(64, 5, strides=2, input_shape=(self.img_rows, self.img_cols, self.channel),    \n",
    "                       padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(0.4))\n",
    "        \n",
    "        # size: 14 X 14 x 64\n",
    "        self.D.add(Conv2D(128, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(0.4))\n",
    "\n",
    "        # size: 7 x 7 x 128\n",
    "        self.D.add(Conv2D(256, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(0.4))\n",
    "        \n",
    "        # size: 4 x 4 x 256\n",
    "        self.D.add(Conv2D(512, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(0.4))\n",
    "\n",
    "        # In_size: 4 x 4 x 512\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        \n",
    "        return self.D\n",
    "\n",
    "    \n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        \n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth --> dense output size = 7x7x256\n",
    "        # Batchnormalization after Dense\n",
    "        # Upsampling2D: shape*2\n",
    "        # Conv2DTranspose: deepth/2\n",
    "        self.G.add(Dense((7*7*256), input_shape=(100,)))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((7, 7, 256)))\n",
    "        self.G.add(Dropout(0.4))\n",
    "    \n",
    "        # Each level after this:\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        \n",
    "        # In 7 x 7 x 256\n",
    "        # out: 14 x 14 x 128\n",
    "        # process: 7 x 7 x 256 -(upsample)-> 14 x 14 x 256 -(convtranspose)-> 14 x 14 x 128\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(128, 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "    \n",
    "        # In: 14 x 14 x 128\n",
    "        # Out:28 x 28 x 64\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(64, 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # In: 28 x 28 x 64\n",
    "        # Out: 28 x 28 x 32\n",
    "        self.G.add(Conv2DTranspose(32, 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_dir = \"./MINST_Images/\"\n",
    "if not os.path.exists(Img_dir):\n",
    "    os.makedirs(Img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = input_data.read_data_sets(\"mnist\", one_hot=True).train.images\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows, self.img_cols, 1).astype(np.float32)\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=5):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0], noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        Img_dir = \"./MINST_Images/\"\n",
    "        if not os.path.exists(Img_dir):\n",
    "            os.makedirs(Img_dir)\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(Img_dir+filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.692636, acc: 0.517578]  [A loss: 1.446547, acc: 0.000000]\n",
      "1: [D loss: 0.632426, acc: 0.996094]  [A loss: 2.712152, acc: 0.000000]\n",
      "2: [D loss: 0.511740, acc: 0.939453]  [A loss: 1.843724, acc: 0.000000]\n",
      "3: [D loss: 0.600859, acc: 0.500000]  [A loss: 8.958580, acc: 0.000000]\n",
      "4: [D loss: 0.426318, acc: 0.701172]  [A loss: 0.107659, acc: 1.000000]\n",
      "5: [D loss: 0.585502, acc: 0.511719]  [A loss: 4.162525, acc: 0.000000]\n",
      "6: [D loss: 0.103945, acc: 0.992188]  [A loss: 0.178662, acc: 0.992188]\n",
      "7: [D loss: 0.121119, acc: 1.000000]  [A loss: 0.314606, acc: 0.917969]\n",
      "8: [D loss: 0.105372, acc: 0.994141]  [A loss: 0.121031, acc: 0.996094]\n",
      "9: [D loss: 0.095060, acc: 0.988281]  [A loss: 0.056521, acc: 1.000000]\n",
      "10: [D loss: 0.089211, acc: 0.996094]  [A loss: 0.069684, acc: 0.996094]\n",
      "11: [D loss: 0.101398, acc: 0.990234]  [A loss: 0.017694, acc: 1.000000]\n",
      "12: [D loss: 0.095427, acc: 0.994141]  [A loss: 0.017649, acc: 1.000000]\n",
      "13: [D loss: 0.099468, acc: 0.988281]  [A loss: 0.010113, acc: 1.000000]\n",
      "14: [D loss: 0.088537, acc: 0.992188]  [A loss: 0.009874, acc: 1.000000]\n",
      "15: [D loss: 0.075619, acc: 0.998047]  [A loss: 0.010294, acc: 1.000000]\n",
      "16: [D loss: 0.072155, acc: 0.996094]  [A loss: 0.001068, acc: 1.000000]\n",
      "17: [D loss: 0.066261, acc: 0.996094]  [A loss: 0.003259, acc: 1.000000]\n",
      "18: [D loss: 0.050790, acc: 1.000000]  [A loss: 0.003360, acc: 1.000000]\n",
      "19: [D loss: 0.049347, acc: 0.996094]  [A loss: 0.000776, acc: 1.000000]\n",
      "20: [D loss: 0.046097, acc: 0.998047]  [A loss: 0.000324, acc: 1.000000]\n",
      "21: [D loss: 0.035048, acc: 1.000000]  [A loss: 0.001892, acc: 1.000000]\n",
      "22: [D loss: 0.037466, acc: 0.992188]  [A loss: 0.000203, acc: 1.000000]\n",
      "23: [D loss: 0.028398, acc: 1.000000]  [A loss: 0.000719, acc: 1.000000]\n",
      "24: [D loss: 0.023524, acc: 1.000000]  [A loss: 0.000188, acc: 1.000000]\n",
      "25: [D loss: 0.029096, acc: 0.994141]  [A loss: 0.000015, acc: 1.000000]\n",
      "26: [D loss: 0.022737, acc: 0.998047]  [A loss: 0.000074, acc: 1.000000]\n",
      "27: [D loss: 0.017224, acc: 1.000000]  [A loss: 0.000067, acc: 1.000000]\n",
      "28: [D loss: 0.013876, acc: 1.000000]  [A loss: 0.000232, acc: 1.000000]\n",
      "29: [D loss: 0.014692, acc: 0.998047]  [A loss: 0.000030, acc: 1.000000]\n",
      "30: [D loss: 0.011142, acc: 1.000000]  [A loss: 0.000364, acc: 1.000000]\n",
      "31: [D loss: 0.010792, acc: 1.000000]  [A loss: 0.000158, acc: 1.000000]\n",
      "32: [D loss: 0.008346, acc: 1.000000]  [A loss: 0.000396, acc: 1.000000]\n",
      "33: [D loss: 0.007682, acc: 1.000000]  [A loss: 0.000638, acc: 1.000000]\n",
      "34: [D loss: 0.009479, acc: 1.000000]  [A loss: 0.000019, acc: 1.000000]\n",
      "35: [D loss: 0.008496, acc: 1.000000]  [A loss: 0.002599, acc: 1.000000]\n",
      "36: [D loss: 0.007834, acc: 1.000000]  [A loss: 0.001543, acc: 1.000000]\n",
      "37: [D loss: 0.013691, acc: 0.996094]  [A loss: 0.000023, acc: 1.000000]\n",
      "38: [D loss: 0.014216, acc: 0.998047]  [A loss: 0.000808, acc: 1.000000]\n",
      "39: [D loss: 0.007349, acc: 1.000000]  [A loss: 0.016025, acc: 0.992188]\n",
      "40: [D loss: 0.073963, acc: 0.986328]  [A loss: 16.118101, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41: [D loss: 5.560041, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "42: [D loss: 1.210302, acc: 0.560547]  [A loss: 0.000004, acc: 1.000000]\n",
      "43: [D loss: 0.050225, acc: 0.988281]  [A loss: 0.000200, acc: 1.000000]\n",
      "44: [D loss: 0.050333, acc: 0.992188]  [A loss: 0.004671, acc: 1.000000]\n",
      "45: [D loss: 0.194332, acc: 0.916016]  [A loss: 0.296316, acc: 0.878906]\n",
      "46: [D loss: 7.253973, acc: 0.500000]  [A loss: 0.000001, acc: 1.000000]\n",
      "47: [D loss: 3.452428, acc: 0.505859]  [A loss: 16.118101, acc: 0.000000]\n",
      "48: [D loss: 6.582989, acc: 0.500000]  [A loss: 0.892385, acc: 0.765625]\n",
      "49: [D loss: 7.588823, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "50: [D loss: 7.340280, acc: 0.500000]  [A loss: 0.000259, acc: 1.000000]\n",
      "51: [D loss: 4.640446, acc: 0.501953]  [A loss: 16.118101, acc: 0.000000]\n",
      "52: [D loss: 6.528356, acc: 0.500000]  [A loss: 8.946106, acc: 0.003906]\n",
      "53: [D loss: 7.862472, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "54: [D loss: 7.879908, acc: 0.498047]  [A loss: 0.000004, acc: 1.000000]\n",
      "55: [D loss: 7.532283, acc: 0.496094]  [A loss: 6.182100, acc: 0.050781]\n",
      "56: [D loss: 7.511283, acc: 0.500000]  [A loss: 0.407419, acc: 0.839844]\n",
      "57: [D loss: 3.976172, acc: 0.488281]  [A loss: 16.118101, acc: 0.000000]\n",
      "58: [D loss: 6.979106, acc: 0.500000]  [A loss: 15.122917, acc: 0.000000]\n",
      "59: [D loss: 0.798346, acc: 0.681641]  [A loss: 6.502196, acc: 0.035156]\n",
      "60: [D loss: 6.562311, acc: 0.496094]  [A loss: 16.097902, acc: 0.000000]\n",
      "61: [D loss: 3.768770, acc: 0.500000]  [A loss: 0.291881, acc: 0.882812]\n",
      "62: [D loss: 4.086996, acc: 0.484375]  [A loss: 16.118101, acc: 0.000000]\n",
      "63: [D loss: 6.098771, acc: 0.500000]  [A loss: 13.018703, acc: 0.000000]\n",
      "64: [D loss: 2.653923, acc: 0.486328]  [A loss: 16.118101, acc: 0.000000]\n",
      "65: [D loss: 5.503908, acc: 0.500000]  [A loss: 10.760251, acc: 0.000000]\n",
      "66: [D loss: 3.590031, acc: 0.488281]  [A loss: 16.058746, acc: 0.000000]\n",
      "67: [D loss: 3.573738, acc: 0.500000]  [A loss: 3.165002, acc: 0.128906]\n",
      "68: [D loss: 5.494716, acc: 0.494141]  [A loss: 14.462373, acc: 0.000000]\n",
      "69: [D loss: 0.717221, acc: 0.712891]  [A loss: 1.683067, acc: 0.359375]\n",
      "70: [D loss: 4.184990, acc: 0.496094]  [A loss: 15.654739, acc: 0.000000]\n",
      "71: [D loss: 2.235875, acc: 0.503906]  [A loss: 1.538372, acc: 0.421875]\n",
      "72: [D loss: 4.273549, acc: 0.498047]  [A loss: 15.733432, acc: 0.000000]\n",
      "73: [D loss: 2.279763, acc: 0.501953]  [A loss: 1.791054, acc: 0.312500]\n",
      "74: [D loss: 4.687383, acc: 0.500000]  [A loss: 14.843680, acc: 0.000000]\n",
      "75: [D loss: 0.930845, acc: 0.654297]  [A loss: 0.859021, acc: 0.636719]\n",
      "76: [D loss: 3.604222, acc: 0.500000]  [A loss: 15.094350, acc: 0.000000]\n",
      "77: [D loss: 1.099196, acc: 0.587891]  [A loss: 1.026987, acc: 0.582031]\n",
      "78: [D loss: 3.693346, acc: 0.500000]  [A loss: 14.422815, acc: 0.000000]\n",
      "79: [D loss: 0.536717, acc: 0.763672]  [A loss: 1.296032, acc: 0.445312]\n",
      "80: [D loss: 3.584521, acc: 0.498047]  [A loss: 14.123222, acc: 0.000000]\n",
      "81: [D loss: 0.658957, acc: 0.748047]  [A loss: 1.240989, acc: 0.421875]\n",
      "82: [D loss: 3.716779, acc: 0.492188]  [A loss: 13.463760, acc: 0.000000]\n",
      "83: [D loss: 0.522460, acc: 0.765625]  [A loss: 2.811595, acc: 0.101562]\n",
      "84: [D loss: 3.399619, acc: 0.500000]  [A loss: 13.515490, acc: 0.000000]\n",
      "85: [D loss: 0.638073, acc: 0.742188]  [A loss: 1.139527, acc: 0.507812]\n",
      "86: [D loss: 2.923167, acc: 0.494141]  [A loss: 13.107629, acc: 0.000000]\n",
      "87: [D loss: 0.583412, acc: 0.765625]  [A loss: 1.251835, acc: 0.433594]\n",
      "88: [D loss: 2.835365, acc: 0.500000]  [A loss: 13.214453, acc: 0.000000]\n",
      "89: [D loss: 0.631990, acc: 0.720703]  [A loss: 0.847778, acc: 0.601562]\n",
      "90: [D loss: 2.483055, acc: 0.498047]  [A loss: 11.741819, acc: 0.000000]\n",
      "91: [D loss: 0.417570, acc: 0.859375]  [A loss: 2.311635, acc: 0.109375]\n",
      "92: [D loss: 2.461125, acc: 0.503906]  [A loss: 13.072433, acc: 0.000000]\n",
      "93: [D loss: 0.629648, acc: 0.753906]  [A loss: 0.887335, acc: 0.546875]\n",
      "94: [D loss: 2.198136, acc: 0.501953]  [A loss: 10.848234, acc: 0.000000]\n",
      "95: [D loss: 0.417092, acc: 0.845703]  [A loss: 2.255317, acc: 0.085938]\n",
      "96: [D loss: 1.974314, acc: 0.505859]  [A loss: 10.175765, acc: 0.000000]\n",
      "97: [D loss: 0.357212, acc: 0.871094]  [A loss: 2.050739, acc: 0.128906]\n",
      "98: [D loss: 1.661414, acc: 0.505859]  [A loss: 8.082673, acc: 0.000000]\n",
      "99: [D loss: 0.407221, acc: 0.802734]  [A loss: 2.675898, acc: 0.035156]\n",
      "100: [D loss: 1.225416, acc: 0.511719]  [A loss: 7.049451, acc: 0.000000]\n",
      "101: [D loss: 0.400052, acc: 0.812500]  [A loss: 2.314857, acc: 0.062500]\n",
      "102: [D loss: 1.116081, acc: 0.511719]  [A loss: 5.645813, acc: 0.000000]\n",
      "103: [D loss: 0.501873, acc: 0.683594]  [A loss: 3.092118, acc: 0.023438]\n",
      "104: [D loss: 0.928986, acc: 0.537109]  [A loss: 5.997488, acc: 0.000000]\n",
      "105: [D loss: 0.400916, acc: 0.792969]  [A loss: 2.072813, acc: 0.078125]\n",
      "106: [D loss: 0.949447, acc: 0.515625]  [A loss: 4.819678, acc: 0.000000]\n",
      "107: [D loss: 0.484863, acc: 0.703125]  [A loss: 2.077999, acc: 0.089844]\n",
      "108: [D loss: 0.842197, acc: 0.535156]  [A loss: 4.748338, acc: 0.000000]\n",
      "109: [D loss: 0.485668, acc: 0.730469]  [A loss: 2.075491, acc: 0.066406]\n",
      "110: [D loss: 0.852773, acc: 0.535156]  [A loss: 4.774722, acc: 0.000000]\n",
      "111: [D loss: 0.468437, acc: 0.746094]  [A loss: 1.497180, acc: 0.246094]\n",
      "112: [D loss: 0.799204, acc: 0.535156]  [A loss: 4.282133, acc: 0.003906]\n",
      "113: [D loss: 0.513128, acc: 0.707031]  [A loss: 1.367610, acc: 0.257812]\n",
      "114: [D loss: 0.885667, acc: 0.535156]  [A loss: 5.225485, acc: 0.000000]\n",
      "115: [D loss: 0.504200, acc: 0.746094]  [A loss: 0.472404, acc: 0.785156]\n",
      "116: [D loss: 0.893943, acc: 0.525391]  [A loss: 4.298335, acc: 0.000000]\n",
      "117: [D loss: 0.600623, acc: 0.644531]  [A loss: 1.394324, acc: 0.218750]\n",
      "118: [D loss: 1.115397, acc: 0.517578]  [A loss: 7.721377, acc: 0.000000]\n",
      "119: [D loss: 0.798592, acc: 0.619141]  [A loss: 0.031161, acc: 1.000000]\n",
      "120: [D loss: 0.950122, acc: 0.511719]  [A loss: 0.507994, acc: 0.746094]\n",
      "121: [D loss: 0.939231, acc: 0.521484]  [A loss: 3.944050, acc: 0.000000]\n",
      "122: [D loss: 0.694309, acc: 0.613281]  [A loss: 1.474841, acc: 0.199219]\n",
      "123: [D loss: 1.036644, acc: 0.517578]  [A loss: 7.677506, acc: 0.000000]\n",
      "124: [D loss: 0.841736, acc: 0.605469]  [A loss: 0.023486, acc: 1.000000]\n",
      "125: [D loss: 1.200263, acc: 0.501953]  [A loss: 0.564157, acc: 0.699219]\n",
      "126: [D loss: 0.937951, acc: 0.519531]  [A loss: 4.311398, acc: 0.000000]\n",
      "127: [D loss: 0.664590, acc: 0.644531]  [A loss: 0.486261, acc: 0.769531]\n",
      "128: [D loss: 1.303448, acc: 0.503906]  [A loss: 8.634286, acc: 0.000000]\n",
      "129: [D loss: 1.133548, acc: 0.527344]  [A loss: 0.023840, acc: 1.000000]\n",
      "130: [D loss: 1.239023, acc: 0.500000]  [A loss: 0.390739, acc: 0.855469]\n",
      "131: [D loss: 0.979975, acc: 0.511719]  [A loss: 3.118794, acc: 0.003906]\n",
      "132: [D loss: 0.721878, acc: 0.554688]  [A loss: 2.293338, acc: 0.015625]\n",
      "133: [D loss: 0.915329, acc: 0.550781]  [A loss: 6.669059, acc: 0.000000]\n",
      "134: [D loss: 0.898223, acc: 0.560547]  [A loss: 0.040427, acc: 1.000000]\n",
      "135: [D loss: 1.791759, acc: 0.500000]  [A loss: 2.336202, acc: 0.015625]\n",
      "136: [D loss: 0.795299, acc: 0.558594]  [A loss: 3.707829, acc: 0.000000]\n",
      "137: [D loss: 0.663199, acc: 0.621094]  [A loss: 1.678665, acc: 0.082031]\n",
      "138: [D loss: 1.135242, acc: 0.505859]  [A loss: 8.641947, acc: 0.000000]\n",
      "139: [D loss: 1.357249, acc: 0.515625]  [A loss: 0.019482, acc: 1.000000]\n",
      "140: [D loss: 1.492023, acc: 0.500000]  [A loss: 0.359570, acc: 0.871094]\n",
      "141: [D loss: 1.058241, acc: 0.507812]  [A loss: 2.846716, acc: 0.003906]\n",
      "142: [D loss: 0.682422, acc: 0.625000]  [A loss: 1.517889, acc: 0.125000]\n",
      "143: [D loss: 1.074538, acc: 0.515625]  [A loss: 6.274798, acc: 0.000000]\n",
      "144: [D loss: 0.913201, acc: 0.572266]  [A loss: 0.036074, acc: 1.000000]\n",
      "145: [D loss: 1.892322, acc: 0.500000]  [A loss: 1.931612, acc: 0.031250]\n",
      "146: [D loss: 0.817934, acc: 0.572266]  [A loss: 3.588543, acc: 0.000000]\n",
      "147: [D loss: 0.757580, acc: 0.576172]  [A loss: 1.364353, acc: 0.179688]\n",
      "148: [D loss: 1.081211, acc: 0.505859]  [A loss: 7.137315, acc: 0.000000]\n",
      "149: [D loss: 1.001354, acc: 0.537109]  [A loss: 0.020375, acc: 1.000000]\n",
      "150: [D loss: 1.972935, acc: 0.500000]  [A loss: 1.181123, acc: 0.203125]\n",
      "151: [D loss: 0.936105, acc: 0.519531]  [A loss: 3.980287, acc: 0.000000]\n",
      "152: [D loss: 0.716492, acc: 0.605469]  [A loss: 0.532724, acc: 0.722656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153: [D loss: 1.259800, acc: 0.501953]  [A loss: 5.764864, acc: 0.000000]\n",
      "154: [D loss: 0.921888, acc: 0.562500]  [A loss: 0.032861, acc: 1.000000]\n",
      "155: [D loss: 1.724062, acc: 0.500000]  [A loss: 1.384669, acc: 0.117188]\n",
      "156: [D loss: 0.934639, acc: 0.513672]  [A loss: 3.728964, acc: 0.000000]\n",
      "157: [D loss: 0.761112, acc: 0.542969]  [A loss: 0.776481, acc: 0.472656]\n",
      "158: [D loss: 1.211694, acc: 0.511719]  [A loss: 6.881843, acc: 0.000000]\n",
      "159: [D loss: 1.107727, acc: 0.529297]  [A loss: 0.018416, acc: 1.000000]\n",
      "160: [D loss: 2.087956, acc: 0.500000]  [A loss: 0.888512, acc: 0.382812]\n",
      "161: [D loss: 1.048052, acc: 0.511719]  [A loss: 4.038708, acc: 0.000000]\n",
      "162: [D loss: 0.686349, acc: 0.621094]  [A loss: 0.364934, acc: 0.890625]\n",
      "163: [D loss: 1.252784, acc: 0.507812]  [A loss: 4.995711, acc: 0.000000]\n",
      "164: [D loss: 0.799382, acc: 0.642578]  [A loss: 0.071707, acc: 1.000000]\n",
      "165: [D loss: 1.687503, acc: 0.500000]  [A loss: 2.449838, acc: 0.000000]\n",
      "166: [D loss: 0.787348, acc: 0.535156]  [A loss: 2.685356, acc: 0.003906]\n",
      "167: [D loss: 0.893679, acc: 0.519531]  [A loss: 4.191948, acc: 0.000000]\n",
      "168: [D loss: 0.717745, acc: 0.587891]  [A loss: 0.658805, acc: 0.613281]\n",
      "169: [D loss: 1.398729, acc: 0.496094]  [A loss: 8.298893, acc: 0.000000]\n",
      "170: [D loss: 1.409214, acc: 0.509766]  [A loss: 0.009147, acc: 1.000000]\n",
      "171: [D loss: 2.003588, acc: 0.500000]  [A loss: 0.328125, acc: 0.914062]\n",
      "172: [D loss: 1.115257, acc: 0.509766]  [A loss: 2.530784, acc: 0.000000]\n",
      "173: [D loss: 0.786826, acc: 0.548828]  [A loss: 1.818730, acc: 0.031250]\n",
      "174: [D loss: 0.999476, acc: 0.501953]  [A loss: 4.922764, acc: 0.000000]\n",
      "175: [D loss: 0.803722, acc: 0.519531]  [A loss: 0.143858, acc: 1.000000]\n",
      "176: [D loss: 1.601147, acc: 0.500000]  [A loss: 5.261796, acc: 0.000000]\n",
      "177: [D loss: 0.865295, acc: 0.583984]  [A loss: 0.031786, acc: 1.000000]\n",
      "178: [D loss: 2.028206, acc: 0.500000]  [A loss: 2.293303, acc: 0.015625]\n",
      "179: [D loss: 0.890376, acc: 0.544922]  [A loss: 3.584617, acc: 0.000000]\n",
      "180: [D loss: 0.857911, acc: 0.503906]  [A loss: 1.814729, acc: 0.058594]\n",
      "181: [D loss: 1.243918, acc: 0.505859]  [A loss: 8.513181, acc: 0.000000]\n",
      "182: [D loss: 1.417647, acc: 0.529297]  [A loss: 0.003896, acc: 1.000000]\n",
      "183: [D loss: 2.704468, acc: 0.500000]  [A loss: 0.491412, acc: 0.789062]\n",
      "184: [D loss: 1.159973, acc: 0.511719]  [A loss: 3.830153, acc: 0.000000]\n",
      "185: [D loss: 0.723844, acc: 0.585938]  [A loss: 0.851267, acc: 0.453125]\n",
      "186: [D loss: 1.309445, acc: 0.490234]  [A loss: 6.399141, acc: 0.000000]\n",
      "187: [D loss: 0.936245, acc: 0.580078]  [A loss: 0.021152, acc: 1.000000]\n",
      "188: [D loss: 2.148229, acc: 0.500000]  [A loss: 1.563698, acc: 0.132812]\n",
      "189: [D loss: 1.033444, acc: 0.509766]  [A loss: 4.339312, acc: 0.000000]\n",
      "190: [D loss: 0.789681, acc: 0.548828]  [A loss: 0.415478, acc: 0.835938]\n",
      "191: [D loss: 1.367444, acc: 0.507812]  [A loss: 6.622458, acc: 0.000000]\n",
      "192: [D loss: 0.965272, acc: 0.576172]  [A loss: 0.015961, acc: 1.000000]\n",
      "193: [D loss: 2.087473, acc: 0.500000]  [A loss: 1.038482, acc: 0.320312]\n",
      "194: [D loss: 1.030777, acc: 0.501953]  [A loss: 3.923904, acc: 0.000000]\n",
      "195: [D loss: 0.773606, acc: 0.570312]  [A loss: 0.806114, acc: 0.468750]\n",
      "196: [D loss: 1.384199, acc: 0.507812]  [A loss: 7.383450, acc: 0.000000]\n",
      "197: [D loss: 1.004976, acc: 0.574219]  [A loss: 0.012077, acc: 1.000000]\n",
      "198: [D loss: 2.306410, acc: 0.500000]  [A loss: 1.141030, acc: 0.257812]\n",
      "199: [D loss: 1.256672, acc: 0.505859]  [A loss: 5.164576, acc: 0.000000]\n",
      "200: [D loss: 0.810097, acc: 0.568359]  [A loss: 0.154506, acc: 0.988281]\n",
      "201: [D loss: 1.556815, acc: 0.498047]  [A loss: 4.834941, acc: 0.000000]\n",
      "202: [D loss: 0.773258, acc: 0.597656]  [A loss: 0.129707, acc: 0.992188]\n",
      "203: [D loss: 1.580699, acc: 0.500000]  [A loss: 4.289236, acc: 0.000000]\n",
      "204: [D loss: 0.728585, acc: 0.607422]  [A loss: 0.409869, acc: 0.824219]\n",
      "205: [D loss: 1.465544, acc: 0.503906]  [A loss: 6.712697, acc: 0.000000]\n",
      "206: [D loss: 0.931258, acc: 0.611328]  [A loss: 0.013098, acc: 1.000000]\n",
      "207: [D loss: 2.186068, acc: 0.500000]  [A loss: 0.896861, acc: 0.394531]\n",
      "208: [D loss: 1.206633, acc: 0.511719]  [A loss: 5.166992, acc: 0.000000]\n",
      "209: [D loss: 0.761397, acc: 0.607422]  [A loss: 0.106723, acc: 0.996094]\n",
      "210: [D loss: 1.674720, acc: 0.500000]  [A loss: 4.205168, acc: 0.000000]\n",
      "211: [D loss: 0.700630, acc: 0.599609]  [A loss: 0.787428, acc: 0.535156]\n",
      "212: [D loss: 1.601934, acc: 0.511719]  [A loss: 8.220073, acc: 0.000000]\n",
      "213: [D loss: 1.047709, acc: 0.580078]  [A loss: 0.008975, acc: 1.000000]\n",
      "214: [D loss: 2.457102, acc: 0.500000]  [A loss: 0.868596, acc: 0.457031]\n",
      "215: [D loss: 1.158531, acc: 0.521484]  [A loss: 4.751500, acc: 0.000000]\n",
      "216: [D loss: 0.704877, acc: 0.617188]  [A loss: 0.588570, acc: 0.671875]\n",
      "217: [D loss: 1.570815, acc: 0.500000]  [A loss: 7.592861, acc: 0.000000]\n",
      "218: [D loss: 0.989834, acc: 0.601562]  [A loss: 0.008168, acc: 1.000000]\n",
      "219: [D loss: 2.583097, acc: 0.500000]  [A loss: 1.157199, acc: 0.273438]\n",
      "220: [D loss: 1.387734, acc: 0.507812]  [A loss: 5.993933, acc: 0.000000]\n",
      "221: [D loss: 0.749997, acc: 0.599609]  [A loss: 0.208827, acc: 0.957031]\n",
      "222: [D loss: 1.689566, acc: 0.500000]  [A loss: 6.133513, acc: 0.000000]\n",
      "223: [D loss: 0.803633, acc: 0.603516]  [A loss: 0.054329, acc: 1.000000]\n",
      "224: [D loss: 2.025548, acc: 0.500000]  [A loss: 3.459497, acc: 0.000000]\n",
      "225: [D loss: 0.881324, acc: 0.564453]  [A loss: 3.533541, acc: 0.000000]\n",
      "226: [D loss: 1.424407, acc: 0.511719]  [A loss: 10.299036, acc: 0.000000]\n",
      "227: [D loss: 1.455405, acc: 0.548828]  [A loss: 0.000763, acc: 1.000000]\n",
      "228: [D loss: 3.978927, acc: 0.500000]  [A loss: 0.380810, acc: 0.851562]\n",
      "229: [D loss: 1.377205, acc: 0.507812]  [A loss: 4.899364, acc: 0.007812]\n",
      "230: [D loss: 0.795213, acc: 0.597656]  [A loss: 0.658983, acc: 0.636719]\n",
      "231: [D loss: 1.796355, acc: 0.507812]  [A loss: 8.067595, acc: 0.000000]\n",
      "232: [D loss: 1.028140, acc: 0.587891]  [A loss: 0.010363, acc: 1.000000]\n",
      "233: [D loss: 2.750748, acc: 0.500000]  [A loss: 2.278313, acc: 0.062500]\n",
      "234: [D loss: 1.398676, acc: 0.494141]  [A loss: 6.878716, acc: 0.000000]\n",
      "235: [D loss: 0.822485, acc: 0.658203]  [A loss: 0.085429, acc: 0.988281]\n",
      "236: [D loss: 1.943203, acc: 0.500000]  [A loss: 4.336595, acc: 0.000000]\n",
      "237: [D loss: 0.875433, acc: 0.542969]  [A loss: 1.838266, acc: 0.125000]\n",
      "238: [D loss: 1.742409, acc: 0.509766]  [A loss: 10.815315, acc: 0.000000]\n",
      "239: [D loss: 1.736652, acc: 0.558594]  [A loss: 0.000448, acc: 1.000000]\n",
      "240: [D loss: 4.569684, acc: 0.500000]  [A loss: 0.369693, acc: 0.855469]\n",
      "241: [D loss: 1.635088, acc: 0.503906]  [A loss: 5.951850, acc: 0.000000]\n",
      "242: [D loss: 0.879034, acc: 0.580078]  [A loss: 0.310079, acc: 0.855469]\n",
      "243: [D loss: 1.763423, acc: 0.503906]  [A loss: 6.817231, acc: 0.000000]\n",
      "244: [D loss: 0.849078, acc: 0.644531]  [A loss: 0.047005, acc: 1.000000]\n",
      "245: [D loss: 2.083288, acc: 0.500000]  [A loss: 4.008075, acc: 0.000000]\n",
      "246: [D loss: 0.930201, acc: 0.560547]  [A loss: 3.215007, acc: 0.007812]\n",
      "247: [D loss: 1.613879, acc: 0.501953]  [A loss: 10.399279, acc: 0.000000]\n",
      "248: [D loss: 1.502425, acc: 0.568359]  [A loss: 0.000369, acc: 1.000000]\n",
      "249: [D loss: 4.673986, acc: 0.500000]  [A loss: 0.379890, acc: 0.828125]\n",
      "250: [D loss: 1.698247, acc: 0.507812]  [A loss: 6.148149, acc: 0.000000]\n",
      "251: [D loss: 0.798393, acc: 0.619141]  [A loss: 0.896241, acc: 0.523438]\n",
      "252: [D loss: 1.857819, acc: 0.505859]  [A loss: 9.379227, acc: 0.000000]\n",
      "253: [D loss: 1.263673, acc: 0.583984]  [A loss: 0.000959, acc: 1.000000]\n",
      "254: [D loss: 4.358539, acc: 0.500000]  [A loss: 1.736575, acc: 0.187500]\n",
      "255: [D loss: 2.133368, acc: 0.501953]  [A loss: 9.438828, acc: 0.000000]\n",
      "256: [D loss: 1.146194, acc: 0.580078]  [A loss: 0.002812, acc: 1.000000]\n",
      "257: [D loss: 3.638319, acc: 0.500000]  [A loss: 2.265671, acc: 0.093750]\n",
      "258: [D loss: 2.010193, acc: 0.501953]  [A loss: 9.101422, acc: 0.000000]\n",
      "259: [D loss: 1.095002, acc: 0.613281]  [A loss: 0.010935, acc: 1.000000]\n",
      "260: [D loss: 3.025820, acc: 0.500000]  [A loss: 4.263849, acc: 0.011719]\n",
      "261: [D loss: 1.385435, acc: 0.519531]  [A loss: 7.350883, acc: 0.000000]\n",
      "262: [D loss: 0.895672, acc: 0.623047]  [A loss: 0.717245, acc: 0.621094]\n",
      "263: [D loss: 2.318932, acc: 0.501953]  [A loss: 11.449280, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264: [D loss: 1.671702, acc: 0.574219]  [A loss: 0.000333, acc: 1.000000]\n",
      "265: [D loss: 4.966663, acc: 0.500000]  [A loss: 0.377504, acc: 0.839844]\n",
      "266: [D loss: 1.802172, acc: 0.515625]  [A loss: 6.256768, acc: 0.000000]\n",
      "267: [D loss: 0.896789, acc: 0.621094]  [A loss: 2.177916, acc: 0.156250]\n",
      "268: [D loss: 2.265734, acc: 0.501953]  [A loss: 12.115791, acc: 0.000000]\n",
      "269: [D loss: 1.804023, acc: 0.556641]  [A loss: 0.000089, acc: 1.000000]\n",
      "270: [D loss: 6.135512, acc: 0.500000]  [A loss: 0.231359, acc: 0.898438]\n",
      "271: [D loss: 2.047202, acc: 0.507812]  [A loss: 7.374921, acc: 0.000000]\n",
      "272: [D loss: 0.959644, acc: 0.593750]  [A loss: 1.644363, acc: 0.285156]\n",
      "273: [D loss: 2.839356, acc: 0.500000]  [A loss: 12.987770, acc: 0.000000]\n",
      "274: [D loss: 2.272991, acc: 0.541016]  [A loss: 0.000073, acc: 1.000000]\n",
      "275: [D loss: 6.956925, acc: 0.500000]  [A loss: 0.048383, acc: 0.992188]\n",
      "276: [D loss: 2.855886, acc: 0.501953]  [A loss: 6.392017, acc: 0.000000]\n",
      "277: [D loss: 1.550042, acc: 0.529297]  [A loss: 7.358267, acc: 0.000000]\n",
      "278: [D loss: 1.443139, acc: 0.519531]  [A loss: 7.770951, acc: 0.000000]\n",
      "279: [D loss: 1.485299, acc: 0.539062]  [A loss: 8.463747, acc: 0.000000]\n",
      "280: [D loss: 1.370797, acc: 0.533203]  [A loss: 5.977772, acc: 0.007812]\n",
      "281: [D loss: 2.323782, acc: 0.509766]  [A loss: 15.801738, acc: 0.000000]\n",
      "282: [D loss: 7.942608, acc: 0.500000]  [A loss: 11.972059, acc: 0.000000]\n",
      "283: [D loss: 1.569274, acc: 0.605469]  [A loss: 0.000086, acc: 1.000000]\n",
      "284: [D loss: 6.301809, acc: 0.500000]  [A loss: 0.425960, acc: 0.824219]\n",
      "285: [D loss: 2.312488, acc: 0.507812]  [A loss: 9.593410, acc: 0.000000]\n",
      "286: [D loss: 1.062170, acc: 0.636719]  [A loss: 0.030237, acc: 0.996094]\n",
      "287: [D loss: 3.093135, acc: 0.500000]  [A loss: 7.394443, acc: 0.000000]\n",
      "288: [D loss: 1.183511, acc: 0.583984]  [A loss: 4.043640, acc: 0.023438]\n",
      "289: [D loss: 3.488683, acc: 0.498047]  [A loss: 15.940053, acc: 0.000000]\n",
      "290: [D loss: 8.051248, acc: 0.500000]  [A loss: 15.165480, acc: 0.000000]\n",
      "291: [D loss: 7.211585, acc: 0.500000]  [A loss: 0.095146, acc: 0.957031]\n",
      "292: [D loss: 2.925855, acc: 0.515625]  [A loss: 10.879324, acc: 0.000000]\n",
      "293: [D loss: 1.395865, acc: 0.644531]  [A loss: 0.004255, acc: 1.000000]\n",
      "294: [D loss: 4.098133, acc: 0.501953]  [A loss: 3.448439, acc: 0.074219]\n",
      "295: [D loss: 3.103654, acc: 0.505859]  [A loss: 13.427326, acc: 0.000000]\n",
      "296: [D loss: 3.339658, acc: 0.533203]  [A loss: 0.000035, acc: 1.000000]\n",
      "297: [D loss: 7.249588, acc: 0.500000]  [A loss: 0.001275, acc: 1.000000]\n",
      "298: [D loss: 5.628287, acc: 0.500000]  [A loss: 1.917857, acc: 0.277344]\n",
      "299: [D loss: 4.376820, acc: 0.496094]  [A loss: 12.731115, acc: 0.000000]\n",
      "300: [D loss: 1.458763, acc: 0.617188]  [A loss: 0.003312, acc: 1.000000]\n",
      "301: [D loss: 5.465550, acc: 0.500000]  [A loss: 4.505327, acc: 0.054688]\n",
      "302: [D loss: 5.040309, acc: 0.503906]  [A loss: 14.737621, acc: 0.000000]\n",
      "303: [D loss: 5.472046, acc: 0.496094]  [A loss: 0.000016, acc: 1.000000]\n",
      "304: [D loss: 7.705631, acc: 0.500000]  [A loss: 0.000630, acc: 1.000000]\n",
      "305: [D loss: 6.989938, acc: 0.500000]  [A loss: 1.368581, acc: 0.601562]\n",
      "306: [D loss: 6.253158, acc: 0.498047]  [A loss: 12.393967, acc: 0.000000]\n",
      "307: [D loss: 3.426849, acc: 0.511719]  [A loss: 15.789252, acc: 0.000000]\n",
      "308: [D loss: 7.957143, acc: 0.500000]  [A loss: 13.644522, acc: 0.000000]\n",
      "309: [D loss: 2.357106, acc: 0.519531]  [A loss: 0.991808, acc: 0.652344]\n",
      "310: [D loss: 6.483644, acc: 0.498047]  [A loss: 14.039008, acc: 0.000000]\n",
      "311: [D loss: 2.682392, acc: 0.486328]  [A loss: 12.263180, acc: 0.000000]\n",
      "312: [D loss: 4.924552, acc: 0.509766]  [A loss: 16.107414, acc: 0.000000]\n",
      "313: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "314: [D loss: 8.057770, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "315: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.096313, acc: 0.000000]\n",
      "316: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.109512, acc: 0.000000]\n",
      "317: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.111473, acc: 0.000000]\n",
      "318: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.106359, acc: 0.000000]\n",
      "319: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n",
      "320: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.109505, acc: 0.000000]\n",
      "321: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.111319, acc: 0.000000]\n",
      "322: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.083454, acc: 0.000000]\n",
      "323: [D loss: 8.059051, acc: 0.500000]  [A loss: 16.027996, acc: 0.000000]\n",
      "324: [D loss: 8.046383, acc: 0.500000]  [A loss: 15.989447, acc: 0.000000]\n",
      "325: [D loss: 8.025251, acc: 0.500000]  [A loss: 14.629402, acc: 0.000000]\n",
      "326: [D loss: 2.776198, acc: 0.535156]  [A loss: 0.012740, acc: 0.996094]\n",
      "327: [D loss: 6.552526, acc: 0.501953]  [A loss: 16.084927, acc: 0.000000]\n",
      "328: [D loss: 8.055633, acc: 0.500000]  [A loss: 16.034195, acc: 0.000000]\n",
      "329: [D loss: 7.990570, acc: 0.500000]  [A loss: 13.883500, acc: 0.007812]\n",
      "330: [D loss: 7.172689, acc: 0.492188]  [A loss: 15.399686, acc: 0.000000]\n",
      "331: [D loss: 4.173637, acc: 0.486328]  [A loss: 0.000924, acc: 1.000000]\n",
      "332: [D loss: 7.697984, acc: 0.498047]  [A loss: 1.389424, acc: 0.738281]\n",
      "333: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000004, acc: 1.000000]\n",
      "334: [D loss: 7.958294, acc: 0.500000]  [A loss: 0.000304, acc: 1.000000]\n",
      "335: [D loss: 7.941913, acc: 0.500000]  [A loss: 0.001122, acc: 1.000000]\n",
      "336: [D loss: 7.940324, acc: 0.500000]  [A loss: 0.000056, acc: 1.000000]\n",
      "337: [D loss: 7.813159, acc: 0.498047]  [A loss: 1.707007, acc: 0.734375]\n",
      "338: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "339: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "340: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000002, acc: 1.000000]\n",
      "341: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "342: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "343: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "344: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "345: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "346: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "347: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "348: [D loss: 7.959926, acc: 0.500000]  [A loss: 0.000001, acc: 1.000000]\n",
      "349: [D loss: 7.957046, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "350: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "351: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "352: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "353: [D loss: 7.936083, acc: 0.500000]  [A loss: 0.013115, acc: 0.992188]\n",
      "354: [D loss: 7.894875, acc: 0.500000]  [A loss: 0.608879, acc: 0.914062]\n",
      "355: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "356: [D loss: 7.971194, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "357: [D loss: 7.971261, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "358: [D loss: 7.941810, acc: 0.500000]  [A loss: 0.374526, acc: 0.945312]\n",
      "359: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "360: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "361: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "362: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "363: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "364: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "365: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "366: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000069, acc: 1.000000]\n",
      "367: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "368: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "369: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "370: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "371: [D loss: 7.971200, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "372: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "373: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "374: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "376: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "377: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "378: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "379: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "380: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "381: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "382: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "383: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "384: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "385: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "386: [D loss: 7.953526, acc: 0.500000]  [A loss: 0.000124, acc: 1.000000]\n",
      "387: [D loss: 7.955447, acc: 0.500000]  [A loss: 2.026796, acc: 0.753906]\n",
      "388: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "389: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "390: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "391: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "392: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "393: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "394: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "395: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "396: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "397: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "398: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "399: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "400: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "401: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "402: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "403: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "404: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "405: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "406: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "407: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "408: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "409: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "410: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "411: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "412: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "413: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "414: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "415: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "416: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "417: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "418: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "419: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "420: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "421: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "422: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "423: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "424: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "425: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "426: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "427: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "428: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "429: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "430: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "431: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "432: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "433: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "434: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "435: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "436: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "437: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "438: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "439: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "440: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "441: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "442: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "443: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "444: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "445: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "446: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "447: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "448: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "449: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "450: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "451: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "452: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "453: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "454: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "455: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "456: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "457: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "458: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "459: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "460: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "461: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "462: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "463: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "464: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "465: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "466: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "467: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "468: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "469: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "470: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "471: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "472: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "473: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "474: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "475: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "476: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "477: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "478: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "479: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "480: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "481: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "482: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "483: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "484: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "485: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "487: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "488: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "489: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "490: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "491: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "492: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "493: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "494: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "495: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "496: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "497: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "498: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "499: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "500: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "501: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "502: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "503: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "504: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "505: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "506: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "507: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "508: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "509: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "510: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "511: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "512: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "513: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "514: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "515: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "516: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "517: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "518: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "519: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "520: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "521: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "522: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "523: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "524: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "525: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "526: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "527: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "528: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "529: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "530: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "531: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "532: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "533: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "534: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "535: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "536: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "537: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "538: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "539: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "540: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "541: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "542: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "543: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "544: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "545: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "546: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "547: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "548: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "549: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "550: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "551: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "552: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "553: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "554: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "555: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "556: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "557: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "558: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "559: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "560: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "561: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "562: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "563: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "564: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "565: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "566: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "567: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "568: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "569: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "570: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "571: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "572: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "573: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "574: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "575: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "576: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "577: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "578: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "579: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "580: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "581: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "582: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "583: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "584: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "585: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "586: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "587: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "588: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "589: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "590: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "591: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "592: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "593: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "594: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "595: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "596: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "598: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "599: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "600: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "601: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "602: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "603: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "604: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "605: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "606: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "607: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "608: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "609: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "610: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "611: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "612: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "613: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "614: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "615: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "616: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "617: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "618: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "619: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "620: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "621: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "622: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "623: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "624: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "625: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "626: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "627: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "628: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "629: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "630: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "631: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "632: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "633: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "634: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "635: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "636: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "637: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "638: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "639: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "640: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "641: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "642: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "643: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "644: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "645: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "646: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "647: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "648: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "649: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "650: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "651: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "652: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "653: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "654: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "655: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "656: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "657: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "658: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "659: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "660: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "661: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "662: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "663: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "664: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "665: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "666: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "667: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "668: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "669: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "670: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "671: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "672: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "673: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "674: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "675: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "676: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "677: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "678: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "679: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "680: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "681: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "682: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "683: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "684: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "685: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "686: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "687: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "688: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "689: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "690: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "691: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "692: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "693: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "694: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "695: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "696: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "697: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "698: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "699: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "700: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "701: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "702: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "703: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "704: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "705: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "706: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "707: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "709: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "710: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "711: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "712: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "713: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "714: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "715: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "716: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "717: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "718: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "719: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "720: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "721: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "722: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "723: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "724: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "725: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "726: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "727: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "728: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "729: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "730: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "731: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "732: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "733: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "734: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "735: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "736: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "737: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "738: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "739: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "740: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "741: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "742: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "743: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "744: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "745: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "746: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "747: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "748: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "749: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "750: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "751: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "752: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "753: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "754: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "755: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "756: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "757: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "758: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "759: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "760: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "761: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "762: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "763: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "764: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "765: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "766: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "767: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "768: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "769: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "770: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "771: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "772: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "773: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "774: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "775: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "776: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "777: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "778: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "779: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "780: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "781: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "782: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "783: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "784: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "785: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "786: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "787: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "788: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "789: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "790: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "791: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "792: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "793: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "794: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "795: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "796: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "797: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "798: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "799: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "800: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "801: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "802: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "803: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "804: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "805: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "806: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "807: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "808: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "809: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "810: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "811: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "812: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "813: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "814: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "815: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "816: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "817: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "818: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "820: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "821: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "822: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "823: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "824: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "825: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "826: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "827: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "828: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "829: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "830: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "831: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "832: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "833: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "834: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "835: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "836: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "837: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "838: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "839: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "840: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "841: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "842: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "843: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "844: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "845: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "846: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "847: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "848: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "849: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "850: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "851: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "852: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "853: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "854: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "855: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "856: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "857: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "858: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "859: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "860: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "861: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "862: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "863: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "864: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "865: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "866: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "867: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "868: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "869: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "870: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "871: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "872: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "873: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "874: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "875: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "876: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "877: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "878: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "879: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "880: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "881: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "882: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "883: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "884: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "885: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "886: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "887: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "888: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "889: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "890: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "891: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "892: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "893: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "894: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "895: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "896: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "897: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "898: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "899: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "900: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "901: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "902: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "903: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "904: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "905: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "906: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "907: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "908: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "909: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "910: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "911: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "912: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "913: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "914: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "915: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "916: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "917: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "918: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "919: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "920: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "921: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "922: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "923: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "924: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "925: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "926: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "927: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "928: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "929: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "931: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "932: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "933: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "934: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "935: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "936: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "937: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "938: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "939: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "940: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "941: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "942: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "943: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "944: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "945: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "946: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "947: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "948: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "949: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "950: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "951: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "952: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "953: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "954: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "955: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "956: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "957: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "958: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "959: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "960: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "961: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "962: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "963: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "964: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "965: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "966: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "967: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "968: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "969: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "970: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "971: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "972: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "973: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "974: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "975: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "976: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "977: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "978: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "979: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "980: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "981: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "982: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "983: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "984: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "985: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "986: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "987: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "988: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "989: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "990: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "991: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "992: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "993: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "994: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "995: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "996: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "997: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "998: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "999: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n"
     ]
    }
   ],
   "source": [
    "mnist_dcgan = MNIST_DCGAN()\n",
    "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
